{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTH 4480 HW4\n",
    "Devon DeJohn, Fall 2019\n",
    "\n",
    "## Exercise 1\n",
    "> Explain the differences between (discrete) data fitting and approximating a given function\n",
    "\n",
    "Data fitting involves approximating a function at all points, whereas approximating a function involves matching exact values of points along an unknown curve.\n",
    "\n",
    "\n",
    "## Exercise 2\n",
    "> What are basis functions? Does an approximant $v(x)$ that is written as a linear combination of basis functions have to be linear in $x$?\n",
    "\n",
    "Basis functions comprise what is called a function space, wherein any function in the space can be represented in terms of linear combinations of the basis functions. The set of all cubic polynomials represents such a function space, and its basis functions can be taken in linear combinations to represent any cubic polynomial that exists. Some arbitrary $v(x)$ then does not necessarily itself need to be a linear function.\n",
    "\n",
    "\n",
    "## Exercise 3\n",
    "> State one advantage and two disadvantages of using the monomial basis for polynomial interpolation.\n",
    "\n",
    "Monomial interpolation is simple to setup, but is computationally expensive. Another major disadvantage is in the relatively limited monomial, where translations along the x-axis cannot occur (this would produce a binomial), e.g., $(x-3)^2.$ This means we have to try to \"reach\" the points in our data set that are, in a sense, \"anchored\" at the origin. This leads to instability, and eventually monomials become useless for interpolating among many data points.\n",
    "\n",
    "\n",
    "## Exercise 4\n",
    "> What are Lagrange Polynomials? How are they used for polynomial interpolation?\n",
    "\n",
    "Lagrange polynomials form another basis from which we can construct linear combinations in order to interpolate between data points.\n",
    "\n",
    "\n",
    "## Exercise 5\n",
    "> State the main advantages and the main disadvantage for using the Lagrange representation.\n",
    "\n",
    "Lagrange interpolation is still straight-forward to setup, if a little tedious. Mainly its strength is in computational efficiency. However, if the spacing is too large between points Lagrange interpolation incurs large errors. Additionally, for evenly spaced points the Lagrange form will tend to oscillate, suffering from something called *Runge's phenomenon.* This is caused in part by the large derivatives you obtain from higher-order polynomials. It is similar to *Gibb's phenomenon* which occurs in Fourier series approximations. \n",
    "\n",
    "\n",
    "## Exercise 6\n",
    "> State two advantages and two disadvantages for using the Newton representation for polynomial interpolation.\n",
    "\n",
    "The construction of Newton polynomials is slightly more computationally expensive than in Lagrange interpolation. Its main advantage is in its adaptability; in the other interpolation methods if a new data point is added the entire polynomial must be reconstructed. Newton's method allows for simply adding the new data point and tacking on another standalone Newton polynomial which includes it.\n",
    "\n",
    "\n",
    "## Exercise 7\n",
    "> Describe the connection between the $kth$ divided difference of a function $f$ and its $kth$ derivatve.\n",
    "\n",
    "When the data points are equispaced, the $kth$ divided difference approximates the $kth$ derivative of the function in the typical quotient rule form for the slope of the function between $x$ and $x+h.$ The smaller the distance between the points, the more accurately approximated is the $kth$ derivative.\n",
    "\n",
    "\n",
    "## Exercise 8\n",
    "> (Monomial) Derive the linear interpolant through the two data points $(1.0,2.0)$ and $(1.1, 2.5).$ Then derive the quadratic interpolant by adding the point $(1.2, 1.5).$ Show a graph depicting this situation\n",
    "\n",
    "Let $M_1(x) = c_0 + c_1 x$ and we have $M_1(1) = c_0 + c_1 = 2$ and $M_1(1.1) = c_0 + 1.1 c_1 = 2.5.$ Let $A$ be the matrix of coefficents on the constants $c_0, c_1,$ and then we can solve the linear equation,\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 \\\\\n",
    "1 & 1.1\n",
    "\\end{bmatrix}\\begin{bmatrix}c_0\\\\c_1\\end{bmatrix} = \\begin{bmatrix}2\\\\2.5\\end{bmatrix} \n",
    "$$\n",
    "\n",
    "$\\det{A} = 0.1,$ and so $A^{-1} = \\begin{bmatrix}11 & -10 \\\\ -10 & 10\\end{bmatrix}$ and then we have\n",
    "\n",
    "\\begin{align}\n",
    "\\begin{bmatrix}11 & -10 \\\\ -10 & 10\\end{bmatrix}\n",
    "\\begin{bmatrix}1 & 1 \\\\1 & 1.1\\end{bmatrix}\n",
    "\\begin{bmatrix}c_0\\\\c_1\\end{bmatrix} &= \\begin{bmatrix}11 & -10 \\\\ -10 & 10\\end{bmatrix}\\begin{bmatrix}2\\\\2.5\\end{bmatrix} \\\\\n",
    "\\begin{bmatrix}c_0\\\\c_1\\end{bmatrix} &= \\begin{bmatrix}22-25\\\\-20 + 25\\end{bmatrix} \\\\\n",
    "\\begin{bmatrix}c_0\\\\c_1\\end{bmatrix} &= \\begin{bmatrix}-3\\\\5\\end{bmatrix}\n",
    "\\end{align}\n",
    "\n",
    "And so $M_1(x) = -3 + 5x.$\n",
    "\n",
    "By the exact same process, we can find the quadratic polynomial $M_2(x) = c_0 + c_1 x + c_2 x^2,$\n",
    "\n",
    "\\begin{align}\n",
    "M_2(1) &= c_0 + c_1 + c_2 = 2 \\\\\n",
    "M_2(1.1) &= c_0 + 1.1 c_1 + 1.21 c_2 = 2.5 \\\\\n",
    "M_2(1.2) &= c_0 + 1.2 c_1 + 1.44 c_2 = 1.5\n",
    "\\end{align}\n",
    "\n",
    "Which gives the matrix equation\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "1 & 1 & 1\\\\\n",
    "1 & 1.1 & 1.21 \\\\\n",
    "1 & 1.2 & 1.44\n",
    "\\end{bmatrix}\n",
    "\\begin{bmatrix} c_0 \\\\ c_1 \\\\ c_2\\end{bmatrix} =\n",
    "\\begin{bmatrix} 2 \\\\ 2.5 \\\\ 1.5 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "With the solution\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix} c_0 \\\\ c_1 \\\\ c_2\\end{bmatrix} =\n",
    "\\begin{bmatrix} -85.5 \\\\ 162.5 \\\\ -75 \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "And the graph of our two monomial interpolants,\n",
    "\n",
    "![](https://i.imgur.com/fJsvoG5.png)\n",
    "\n",
    "When I was checking to make sure I had calculated my matrix equations properly, I saw that even with these simple calculations, computers immediately incur roundoff error:\n",
    "\n",
    "```julia\n",
    "julia> a = [1 1; 1 1.1]\n",
    "2Ã—2 Array{Float64,2}:\n",
    " 1.0  1.0\n",
    " 1.0  1.1\n",
    "\n",
    "julia> b = [2 ; 2.5]\n",
    "2-element Array{Float64,1}:\n",
    " 2.0\n",
    " 2.5\n",
    "\n",
    "julia> a\\b\n",
    "2-element Array{Float64,1}:\n",
    " -2.9999999999999956\n",
    "  4.999999999999996\n",
    "```\n",
    "\n",
    "\n",
    "## Exercise 9\n",
    "> (Lagrange) Use the known values of the function $\\sin(x)$ at $x=0,\\frac{\\pi}{6}, \\frac{\\pi}{4}, \\frac{\\pi}{3}, \\frac{\\pi}{2}$ to derive an interpolating polynomial $p(x).$ What is the degree of your polynomial? What is the interpolation error magnitude $|p(1.2) - \\sin(1.2)|$?\n",
    "\n",
    "Since the first coefficient is $f(x_0)=0,$ we will have $\\mathscr{L}_0=0$ and so I will not bother constructing the basis polynomial for that term. Our Lagrange polynomials are:\n",
    "\n",
    "\\begin{align}\n",
    "y_0 &= 0 &\\mathscr{L}_0 &= 0 \\\\\n",
    "y_1 &= \\frac{1}{2} &\\mathscr{L}_1 &= \\frac{(x-0)(x-\\frac{\\pi}{4})(x-\\frac{\\pi}{3})(x-\\frac{\\pi}{2})}{(\\frac{\\pi}{6}-0)(\\frac{\\pi}{6}-\\frac{\\pi}{4})(\\frac{\\pi}{6}-\\frac{\\pi}{3})(\\frac{\\pi}{6}-\\frac{\\pi}{2})} \\\\\n",
    "y_2 &= \\frac{\\sqrt{2}}{2} &\\mathscr{L}_2 &= \\frac{(x-0)(x-\\frac{\\pi}{6})(x-\\frac{\\pi}{3})(x-\\frac{\\pi}{2})}{(\\frac{\\pi}{4}-0)(\\frac{\\pi}{4}-\\frac{\\pi}{6})(\\frac{\\pi}{4}-\\frac{\\pi}{3})(\\frac{\\pi}{4}-\\frac{\\pi}{2})} \\\\\n",
    "y_3 &= \\frac{\\sqrt{3}}{2} &\\mathscr{L}_3 &= \\frac{(x-0)(x-\\frac{\\pi}{6})(x-\\frac{\\pi}{4})(x-\\frac{\\pi}{2})}{(\\frac{\\pi}{3}-0)(\\frac{\\pi}{3}-\\frac{\\pi}{6})(\\frac{\\pi}{3}-\\frac{\\pi}{4})(\\frac{\\pi}{3}-\\frac{\\pi}{2})} \\\\\n",
    "y_4 &= 1 &\\mathscr{L}_4 &= \\frac{(x-0)(x-\\frac{\\pi}{6})(x-\\frac{\\pi}{4})(x-\\frac{\\pi}{3})}{(\\frac{\\pi}{2} - 0)(\\frac{\\pi}{2} - \\frac{\\pi}{6})(\\frac{\\pi}{2} - \\frac{\\pi}{4})(\\frac{\\pi}{2} - \\frac{\\pi}{3})}\n",
    "\\end{align}\n",
    "\n",
    "This will result in the fourth-degree polynomial (I used Wolfram Alpha to expand and simplify the basis polynomials),\n",
    "\n",
    "\\begin{align}\n",
    "p(x) = \\frac{1}{2}\\left(\\frac{54 x}{\\pi} - \\frac{486 x^2}{\\pi^2} + \\frac{1404 x^3}{\\pi^3} - \\frac{1296 x^4}{\\pi^4}\\right) &+ \\frac{\\sqrt{2}}{2}\\left( -\\frac{64 x}{\\pi} + \\frac{704 x^2}{\\pi^2} - \\frac{2304 x^3}{\\pi^3} + \\frac{2304 x^4}{\\pi^4} \\right) + \\\\\n",
    "\\frac{\\sqrt{3}}{2}\\left( \\frac{27 x}{\\pi} - \\frac{324 x^2}{\\pi^2} + \\frac{1188 x^3}{\\pi^3} - \\frac{1296 x^4}{\\pi^4}\\right) &+ 1\\left( -\\frac{2 x}{\\pi} + \\frac{26 x^2}{\\pi^2} - \\frac{108 x^3}{\\pi^3} + \\frac{144 x^4}{\\pi^4}\\right)\n",
    "\\end{align}\n",
    "\n",
    "Evaluating $p(x)$ at $x=1.2$ gives $0.9321449$ and so the error is $|p(1.2)-\\sin(1.2)| = 0.000105814.$\n",
    "\n",
    "## Exercise 10\n",
    "> (Divided Differences) For some function $f$, you have a table of divided differences of the form\n",
    "\n",
    "| $i$ | $z_i$    | $f[\\cdot]$    | $f[\\cdot, \\cdot]$ | $f[\\cdot, \\cdot, \\cdot]$ | $f[\\cdot, \\cdot, \\cdot, \\cdot]$ |\n",
    "| :-: | :---: | :------: | :-----------: | :----------------: | :-: |\n",
    "| $0$ | $5.0$ | $f[z_0]$ |               |                    | |\n",
    "| $1$ | $5.0$ | $f[z_1]$ | $f[z_0, z_1]$ |                    | |\n",
    "| $2$ | $6.0$ | $4.0$    | $5.0$         | $-3.0$             | |\n",
    "| $3$ | $4.0$ | $2.0$    | $f[z_2, z_3]$ | $f[z_1, z_2, z_3]$ | $f[z_0, z_1, z_2, z_3]$ |\n",
    "\n",
    "> Then construct the polynomial approximation using divided differences.\n",
    "\n",
    "\n",
    "Completing the table (and noting that the first divided difference is actually an exact derivative),\n",
    "\n",
    "| $i$ | $z_i$    | $f[z_i]$    | $f[z_{i-1}, z_i]$ | $f[z_{i-2}, z_{i-1}, z_i]$ | $f[z_{i-3}, z_{i-2}, z_{i-1}, z_i]$ |\n",
    "| :-: | :---: | :------: | :-----------: | :----------------: | :----: |\n",
    "| $0$ | $5.0$ | $-1.0$ |               |                    |        |\n",
    "| $1$ | $5.0$ | $-1.0$ | $8.0$         |                    |        |\n",
    "| $2$ | $6.0$ | $4.0$    | $5.0$         | $-3.0$             |        |\n",
    "| $3$ | $4.0$ | $2.0$    | $1.0$         | $4.0$              | $-7.0$ |\n",
    "\n",
    "\n",
    "And the resulting polynomial,\n",
    "\n",
    "\\begin{align}\n",
    "p(x) &= -1(x-0)+8(x-0)(x-5)-3(x-0)(x-5)(x-6)-7(x-0)(x-5)(x-6)(x-4) \\\\\n",
    "&= -7 x^4 + 102 x^3 - 477 x^2 + 709 x\n",
    "\\end{align}\n",
    "\n",
    "## Exercise 11\n",
    "> Comment on your results in the previous three problems.\n",
    "\n",
    "Lagrange is a pain to do by hand. Monomial for a $2\\times2$ matrix was very simple and quick but obviously intractible for larger matrices. Divided differences is quite simple to put together, if a little tedious (and of course this felt like a bit of a trick question so it took me a while to understand how to fill in the last value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
